\documentclass[12]{beamer}

\usetheme{CambridgeUS}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}

\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\addbibresource{bibliography.bib}

\title{Transaction Concepts}
\date

\begin{document}

\maketitle

\section{Motivation}

\begin{frame}
  \frametitle{Motivation for Transactions}
  \begin{block}{Reliability}
    Database should continue to operate in spite of failures
  \end{block}
  \begin{block}{Consistency}
    Database should not be in an inconsistent state after failure
  \end{block}
\end{frame}

%
% 2. What is a transaction
%
\begin{frame}
  \frametitle{What is a Transaction}
  \begin{definition}
    A basic unit of consistent and reliable computing (\"Ozsu and Valduriez)
    Proved to be the major paradigm for synchronization and recovery (Haerder and Reuter)
  \end{definition}
\vspace{5mm}
\end{frame}

% 3
\begin{frame}
  \frametitle{Isolation and Consistency}
  \begin{itemize}
    \item   The concepts 'Isolation' comes into picture when multiple transactions (from the same user or different users) have to be executed in parallel
    \item Two operations (transactions) conflict if they operate on the same data and at least one of them is a write
    \item Techniques that achieve isolation are known as \textit{synchronization} or \textit{Concurrency Control Techniques}
  \end{itemize}
\end{frame}

\section{Concurrency Control Protocols}

\subsection{Locking}

\begin{frame}
  \frametitle{Locking - Most popular mechanism to attain consistency}
  \begin{itemize}
     \addtolength{\itemsep}{0.5\baselineskip}
    \item Each transaction (operation) reserves access to the data it uses. This reservation is called a \textbf{lock}
    \item The intuition behind locking is that it avoids interference between conflicting operations by using locks to synchronize the execution order
    \item There are \textbf{read locks} and \textbf{write (exclusive) locks}
    \item Before reading a piece of data, a transaction sets a read lock. Before writing the data, it sets a write lock
    \item Read locks conflict with write locks, and write locks conflict with both read and write locks
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Implications of Locking}
  \begin{itemize}
    \item  Locking affects performance. When a transaction sets a lock, it delays other transactions that need to set a conflicting lock.
    \item the more transactions that are running concurrently, the more likely that such delays will happen
    \item locking also affects correctness. For example, if 'serializability' is the goal, then simply locking data before accessing it is not enough. Timing of unlocking too matters.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Two-Phase Locking (2PL) }
  \begin{itemize}
  \item The locking rule that guarantees serializable executions
  \item A transaction must obtain all locks before releasing any of them. Equivalently, a transaction cannot obtain new locks once it starts releasing its locks
  \item 2PL has two phases: \textit{growing phase} during which locks are acquired, and \textit{shrinking phase} during which locks are released
  \item The operation that separates the two phases is the transaction's first unlock operation, which is the first operation of the second phase
  \item Despite the simple intuition, there are no simple proofs of two-phase locking theorem
  \end{itemize}
   \begin{block}{Two-Phase Locking Theorem}
     If all transactions in an execution are two-phase locked, then the execution of serializable.
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Aspects of Locking}
  \begin{itemize}
	\item Automatic Locking
	\item Granularity
	\item Multigranularity Locking
	\item Deadlocks
	\item Deadlock detection
	\item Performance
  \end{itemize}
\end{frame}

\subsubsection{Granularity}

\begin{frame}
  \frametitle{Locking Aspects - Granularity}
  \begin{definition}The size of data items that the TM locks is called \textbf{locking granularity}\end{definition}
  \begin{itemize}
   \addtolength{\itemsep}{1.0\baselineskip}
    \item \textit{Coarse Granularity-} at the level of files, tables etc.,
    \item \textit{Fine Granularity -} at the level of records, fields ...
    \item \textit{Intermediate Granularity -} at the level of pages
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Granularity Tradeoff - Concurrency Vs Locking Overhead}
  \begin{itemize}
% \addtolength{\itemsep}{0.25\baselineskip}
    \item At coarse granularity, fewer locks need to be set, minimizing locking overhead. However, coarse granularity typically results in locking of more data than needed, decreasing concurrency and throughput
    \item At fine granularity, only the required data will be locked. However, it can result in significant locking overhead when locks have to be obtained on a large number of data items
    \item One compromise is to lock at page granularity, resulting in a moderate degree of concurrency with a moderate amount of overhead.  It also simplifies recovery algorithms for commit and abort, since log and data flush is done in units of pages.
    \item For high performance TP, record logging is needed as concurrent transactions need to lock different records on the same page.
  \end{itemize}
\end{frame}

\subsubsection{Multigranularity}

\begin{frame}
  \frametitle{Multigranularity Locking}
  \begin{itemize}
    \item For better performance, TMs lock data object at different granularities - file-level, page-level, record-level etc., This can lead to conflicts. A lock at the record level can conflict with a lock on the enclosing page. The lock manager has no idea that locks at different granularities might conflict
    \item The solution is \textbf{\textit{multigranular locking}}
    \item In this approach, ordinary locks are set at fine grain and \textbf{intention locks} are set at coarse grain
    \item For eg., a write lock is set on a record and a 'write intention' lock set on the enclosing page
    \item Each coarse grain intention lock warns other transactions (locking at coarse granularity) about potential conflict with fine grain locks
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Lock Type Compatibility Matrix}
  \begin{center}
  \scalebox{1.5}{
    \begin{tabular}{ c | c  c  c  c  c | }
      \multicolumn{1}{c}{} & \multicolumn{1}{c}{r} & \multicolumn{1}{c}{w} & \multicolumn{1}{c}{ir} & \multicolumn{1}{c}{iw} & \multicolumn{1}{c}{riw} \\\cline{2-6}
     r & y & n & y & n & n \\
    w & n & n & n & n & n \\
    ir & y & n & y & y & y \\
    iw & n & n & y & y & n \\
    riw & n & n & y & n & n \\\cline{2-6}
    \end{tabular}}
  \end{center}
\end{frame}

\begin{frame}
	\frametitle{Locking - Performance Issues}
  Reduce Lock Contention, by separating writable and readable parts of a record
\end{frame}

\begin{frame}
\frametitle{Locking - Preventing HotSpots}
\begin{itemize}
\item Delay operations until commit. During the course of 2PL, write locks can be acquired late into the transaction, towards the end of read phase. However, read locks will have to be acquired when the data is first read
\item Optimistic Concurrency Control methods
\item partitioning of data, so that different parts of data written to by different transactions are not clubbed together, forcing all transaction to acquire lock on the whole data
\item Batching - Batching of updates across transactions could be explored, although it has the flip side of more work during error recovery
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Locking - Downside and Alternatives}
Locking comes with a penalty. Locking soon becomes a bottleneck, reducing concurrency and transactions' throughput. This is particular accentuated in 'Query-Update' scenario.
  \begin{block}{Query Workloads}
    \begin{itemize}
      \item Read-only requests for decision support and reporting
      \item Run much longer than update transactions and access a lot of data
      \item If run using 2PL, they set many locks and hold them for a long time
      \item This leads to starvation of update transactions
      \item Alternatives: \textbf{Data Warehousing, Weaker Consistency Guarantees, Multi Versioned Data (MVD)}
    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
\frametitle{Consistency and Isolation Levels}
Conceptually, there is a mapping (association) between 'degrees of consistency' and 'isolation levels'. For example, the isolation level 'serializability' provides 'strong consistency'. The level of consistency in turn influences the concurrency control protocol used to achieve the level of consistency in question.
\end{frame}

\begin{frame}
\frametitle{Different Consistency/Isolation Levels}
\begin{itemize}
\item Read Uncommitted (reads dirty, uncommitted data too, no locks held)
\item Read Committed (reads only data committed by an earlier transaction, takes a read lock, but releases it as soon as the data is read)
\item Cursor Stability
\item No Phantom reads
\item Snapshot Isolation (SI)
\item Serializable (uses 2PL to achieve isolation guarantees)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Read Uncommitted (Degree 1 Isolation)}
\begin{itemize}
\item also known as 'Dirty Read'
\item reads uncommitted data
\item increases concurrency (throughput) at the cost of inconsistencies in the read data
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Read Committed (Degree 2 Isolation)}
\begin{itemize}
\item ensures that the query only reads data written by committed transactions
\item achieved by holding the read lock on data items being read. This prevents dirty reads (reading of data written by uncommitted transactions)
\item does not ensure serializability
\item under this isolation, a transaction that reads the same data more than once might read different values for each of the read of a data item
\item in other words, it allows non-repeatable reads (may get different values each time a transaction executes the read)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Isolation Level - Cursor Stability}
\begin{itemize}
\item Stronger version of "Read Committed Isolation"
\item This is applicable during the read (and write) of multiple rows. The read lock is held till the cursor moves to the next row, and not released immediately. Thus the row that the cursor identifies is stable.
\item Should the transaction decide to write the current row, the read lock can be promoted to write lock, and hence the race condition to update the same row by multiple transactions is obviated (and prevents lost updates)
\end{itemize}
\end{frame}

\subsubsection{Multi Version Data (MVD)}

\begin{frame}
\frametitle{Multiversioned Data (MVD)}
\begin{itemize} 
\item A good technique for ensuring that queries read consistent data without slowing down update queries
\item It may be recalled that read committed isolation level requires that read locks be held so that only data written by committed transactions is read by the transaction.
\item With multiversioned data, updates do not overwrite existing copies of data items. Instead, they create a new copy (version) of the data
\item Hence, each item consists of a sequence of versions, each corresponding to each update that was applied to it
\item each version is tagged by the id of the transaction that wrote it
\item all version of a data item are linked in a chain, with each version pointing to the previous version, beginning with the most recent, and going back in time.
\item TMs maintain the list of committed transactions, called the \textbf{commit list}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{MVD Capabilities - Snapshot Mode}
\begin{itemize}
\item allows a query to avoid setting read locks and thereby avoid locking delays
\item when a query (both read-only queries and updaters) executes in this mode, the TM associates the current commit list with the query for the duration of the execution.
\item When the query asks to read a data item, the TM selects the latest version of the data item that is tagged by a transaction ID on the query's commit list. There is no need to lock this data as it can't change. Updaters create new versions and do not overwrite previous versions.
\item locks will have to be acquired on items being written to (2PL)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Snapshot Isolation (SI) - A popular variation of Snapshot Mode}
\begin{itemize}
\item In this variation, an update transaction does not use 2PL. Instead, it executes reads using snapshot mode, and executes writes without setting locks.
\item When an update transaction T commits, the TM checks that T's updates are still valid, that no other committed transaction had written to the same data item while T is running.
\item T is aborted if the TM realizes that another T' had written to the same data item and committed. This is called "first committer wins".
\item Snapshot isolation provides stronger synchronization that read "read committed" isolation. For example, it prevents lost update problem, when two transactions read the same value of a data item and writes one after the other, overwriting the first update
\item However, SI does not provide serializability
\item In principle, MVD also offers "read commit isolation", as the data read was committed by an earlier transaction
\end{itemize}
\end{frame}

\subsection{Optimistic Concurrency Control}

\begin{frame}
\frametitle{Two-Phase Commit (2PC)}
2PC can also be seen as a consensus protocol. Should we commit/abort the distributed transaction ?
\end{frame}

\begin{frame}
\frametitle{2PC versus Paxos (Consensus Protocols)}
random citation \cite{Bernstein:2009}
2PC blocks if the TM fails, requiring human intervention to restart. 3PC algorithms try to fix 2PC by electing a new TM when the original manager fails.
\vspace{10pt}

Consensus protocols, including Paxos do not block as long as a majority of processes (managers) are available. Paxos solves the more general problem of consensus, and hence can be used to implement distributed transaction commit as well. However, in comparison to 2PC, Paxos is more chatty and requires more messages. In comparison to most 3PC algorithms, Paxos renders a simpler, more efficient algorithm, in terms of minimal message delay.
\end{frame}

\section{Error Recovery}

\begin{frame}
  \frametitle{Undo Log}
\end{frame}

\begin{frame}
  \frametitle{References}
  \printbibliography
\end{frame}

\end{document}